{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Initial Exploratory Data Analysis (EDA) \n",
    "print(\"--- Data Loading & Initial EDA ---\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('data/Pumpkin_Seeds_Dataset.csv', encoding='latin1')# this because the dataset contains non-ASCII characters instead of UTF-8\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Pumpkin_Seeds_Dataset.csv' not found.\")\n",
    "    print(\"Please ensure the dataset is in a 'data' subdirectory in the same directory as the script.\")\n",
    "    exit() # Exit the script if the file is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the DataFrame to get a sense of the data structure.\n",
    "print(\"\\n--- Head of the dataset ---\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\") # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print concise information about the DataFrame, including data types and non-null counts.\n",
    "print(\"--- Dataset Info ---\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics of the numerical columns (mean, std, min, max, quartiles).\n",
    "print(\"--- Dataset Description ---\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the 'Class' column to check class distribution.\n",
    "print(\"--- Class Distribution ---\")\n",
    "print(df['Class'].value_counts())\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store original column names before separating X and y for later reference\n",
    "original_feature_names = df.drop(columns=['Class']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# Separate features (X) and the target variable (y) from the original DataFrame\n",
    "X_original = df.drop(columns=['Class'])\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test-train split (using the original feature set)\n",
    "# 80% of the data is used for training (test_size = 0.2 means 20% for testing).\n",
    "# random_state = 42 ensures reproducibility of the split.\n",
    "X_train_original, X_test_original, y_train, y_test = train_test_split(\n",
    "    X_original, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling (using the original feature set)\n",
    "scaler_original = StandardScaler() # this generates a mean of 0 and std of 1 for each feature.\n",
    "X_train_scaled_original = scaler_original.fit_transform(X_train_original) # always fit on the training data only.\n",
    "# Transform the test data using the *same* scaler fitted on the training data.\n",
    "X_test_scaled_original = scaler_original.transform(X_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train and Evaluate the Initial Model (all features) ---\n",
    "print(\"--------- Train and Evaluate the Initial Model (all features) --------\")\n",
    "# Initialize the Logistic Regression model.\n",
    "model_original = LogisticRegression()\n",
    "# Train the model using the scaled training data.\n",
    "model_original.fit(X_train_scaled_original, y_train)\n",
    "# Make predictions on the scaled test data.\n",
    "y_pred_original = model_original.predict(X_test_scaled_original)\n",
    "print(\"--- model successfully trained and predictions made on the test set---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's performance using common metrics at the default threshold (0.5).\n",
    "print(\"--- Model Evaluation (Original Data - Default Threshold) ---\")\n",
    "accuracy_original = metrics.accuracy_score(y_test, y_pred_original)\n",
    "classification_report_original = metrics.classification_report(y_test, y_pred_original)\n",
    "confusion_matrix_original = metrics.confusion_matrix(y_test, y_pred_original)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_original)\n",
    "print(\"Classification Report:\\n\", classification_report_original)\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_original)\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Confusion Matrix heatmap (Original Data)\n",
    "cm_original = confusion_matrix(y_test, y_pred_original)\n",
    "labels_original = model_original.classes_\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_original, annot=True, fmt='d', cmap='Greens', xticklabels=labels_original, yticklabels=labels_original)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Logistic Regression (Original Features)')\n",
    "plt.savefig('confusion_matrix_logistic_regression_original.png')\n",
    "# plt.show() # Uncomment this line if you are running this in an environment that displays plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUC (Original Data)\n",
    "print(\"--- ROC AUC Analysis (Original Model) ---\")\n",
    "# Convert the true test labels to binary format (0 and 1).\n",
    "lb = LabelBinarizer()\n",
    "y_test_bin = lb.fit_transform(y_test).ravel() # Flatten the array to 1D\n",
    "\n",
    "# Get the predicted probabilities for the positive class from the original model.\n",
    "y_probs_original = model_original.predict_proba(X_test_scaled_original)[:, 1]\n",
    "\n",
    "# Compute and print the ROC AUC score.\n",
    "auc_score_original = roc_auc_score(y_test_bin, y_probs_original)\n",
    "print(f\"ROC AUC Score (Original Model): {auc_score_original:.3f}\")\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "# Plot Predicted Score Distributions (Original Data)\n",
    "# Separate the predicted probabilities based on the actual class.\n",
    "neg_scores_original = y_probs_original[y_test_bin == 0] # Probabilities for actual negative instances\n",
    "pos_scores_original = y_probs_original[y_test_bin == 1] # Probabilities for actual positive instances\n",
    "class_names_original = model_original.classes_ # Get the actual class names so i can display them in the plot.\n",
    "neg_class_name_original = class_names_original[0] \n",
    "pos_class_name_original = class_names_original[1] \n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.kdeplot(neg_scores_original, shade=True, label= (f\"-{neg_class_name_original}\"))\n",
    "sns.kdeplot(pos_scores_original, shade=True, label= (f\"+{pos_class_name_original}\"))\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density') \n",
    "plt.title('Predicted Score Distributions (KDE) - Original Features') \n",
    "plt.legend() \n",
    "plt.savefig('predicted_score_distributions_original.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Interpret the Coefficients of the Initial Model ---\n",
    "print(\"--- Interpret the Coefficients of the Initial Model ---\")\n",
    "\n",
    "# Get the intercept (log-odds of positive class when all features are 0)\n",
    "intercept_original = model_original.intercept_[0] \n",
    "\n",
    "# Get the coefficients for each feature\n",
    "coefficients_original = model_original.coef_[0]\n",
    "\n",
    "print(\"--- Model Intercept (Original Model) ---\")\n",
    "print(f\"Intercept: {intercept_original:.4f}\")\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "print(\"--- Model Coefficients (Original Model) ---\")\n",
    "# Print each feature name and its corresponding coefficient\n",
    "for feature, coef in zip(original_feature_names, coefficients_original):\n",
    "    print(f\"Feature: {feature:<20} | Coefficient: {coef:.4f}\")\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"--- Interpretation and Next Steps ---\")\n",
    "print(\"The coefficients indicate the change in the log-odds of the positive class (Ürgüp Sivrisi)\")\n",
    "print(\"for a one-unit increase in the scaled feature value, holding other features constant.\")\n",
    "print(\"Features with  coefficients close to zero have a weaker linear association.\")\n",
    "print(\"Based on these results, 'Major_Axis_Length' (-0.0983) and 'Extent' (0.0963) have the smallest absolute coefficients.\")\n",
    "print(\"We will now explore if removing these features impacts the model's performance.\")\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "print(\"--- Dropping Features with Small Coefficients ---\")\n",
    "columns_to_drop = ['Major_Axis_Length', 'Extent']\n",
    "\n",
    "print(f\"Attempting to drop columns: {columns_to_drop}\")\n",
    "\n",
    "# Create a new DataFrame by dropping the specified columns from the original DataFrame (df)\n",
    "# This ensures we start from the original data structure before dropping.\n",
    "try:\n",
    "    df_reduced = df.drop(columns=columns_to_drop)\n",
    "    print(\"\\n-------df after droping columns-------\")\n",
    "    print(df_reduced.columns)\n",
    "except KeyError as e:\n",
    "    print(f\"\\nError dropping columns: {e}\")\n",
    "    print(\"Please ensure the column names to drop are correct and exist in the DataFrame.\")\n",
    "    print(\"Exiting script.\")\n",
    "    exit() # Exit the script if columns cannot be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train and Evaluate the Second Model (reduced features) ---\n",
    "print(\"\\n--- Train and Evaluate the Second Model without the dropped columns(features) ---\")\n",
    "\n",
    "# rename your X variable to avoid confusion with the original X\n",
    "X_reduced = df_reduced.drop(columns=['Class']) # df_reduced is our dataframe we work with now.\n",
    "# y remains the same as it was not affected by dropping feature columns\n",
    "\n",
    "# Use the same random_state as before for consistency in the split\n",
    "X_train_reduced, X_test_reduced, y_train, y_test = train_test_split(\n",
    "    X_reduced, y, test_size=0.2, random_state=42\n",
    ")\n",
    "# y remains the same as it was not affected by dropping feature columns\n",
    "\n",
    "# Need a new scaler for the reduced feature set as the number of features changed\n",
    "scaler_reduced = StandardScaler()\n",
    "X_train_scaled_reduced = scaler_reduced.fit_transform(X_train_reduced)\n",
    "X_test_scaled_reduced = scaler_reduced.transform(X_test_reduced)\n",
    "\n",
    "# Train the Logistic Regression model on the reduced data\n",
    "model_reduced = LogisticRegression()\n",
    "model_reduced.fit(X_train_scaled_reduced, y_train)\n",
    "y_pred_reduced = model_reduced.predict(X_test_scaled_reduced)\n",
    "\n",
    "# --- Compare the performance of the two models (at default threshold) ---\n",
    "print(\"--- Comparison of Model Performance (Default Threshold) ---\")\n",
    "\n",
    "# Evaluate the reduced model at the default threshold\n",
    "accuracy_reduced = metrics.accuracy_score(y_test, y_pred_reduced)\n",
    "classification_report_reduced = metrics.classification_report(y_test, y_pred_reduced)\n",
    "confusion_matrix_reduced = metrics.confusion_matrix(y_test, y_pred_reduced)\n",
    "\n",
    "print(\"--- Evaluation for Original Model (Default Threshold) ---\")\n",
    "print(\"Accuracy:\", accuracy_original)\n",
    "print(\"Classification Report:\\n\", classification_report_original)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_original)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"--- Evaluation for Reduced Model (Default Threshold) ---\")\n",
    "print(\"Accuracy:\", accuracy_reduced)\n",
    "print(\"Classification Report:\\n\", classification_report_reduced)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix_reduced)\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Confusion Matrix heatmap (Reduced Data) for visual comparison\n",
    "cm_reduced = confusion_matrix(y_test, y_pred_reduced)\n",
    "labels_reduced = model_reduced.classes_\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_reduced, annot=True, fmt='d', cmap='Blues', xticklabels=labels_reduced, yticklabels=labels_reduced)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Logistic Regression (Reduced Features)')\n",
    "plt.savefig('confusion_matrix_logistic_regression_reduced.png')\n",
    "# plt.show() # Uncomment to display plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Perform Threshold Optimization (on the Reduced Model) ---\n",
    "print(\"--- 7. Threshold Optimization (on the Reduced Model) ---\")\n",
    "print(\"We will now find the optimal classification threshold for the reduced model\")\n",
    "print(\"to maximize the F1-score.\")\n",
    "\n",
    "# Get the predicted probabilities for the positive class from the reduced model\n",
    "y_probs_reduced = model_reduced.predict_proba(X_test_scaled_reduced)[:, 1]\n",
    "\n",
    "# Calculate ROC AUC for the reduced model\n",
    "auc_score_reduced = roc_auc_score(y_test_bin, y_probs_reduced)\n",
    "print(f\"ROC AUC Score (Reduced Model): {auc_score_reduced:.3f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# choosing the optimal threshold for classification (on Reduced Data)\n",
    "thresholds = np.linspace(0, 1, 101) # Check 101 thresholds between 0 and 1\n",
    "f1_scores_reduced = []\n",
    "# Iterate through each threshold to calculate the F1 score.\n",
    "for t in thresholds:\n",
    "    # Classify instances based on the current threshold.\n",
    "    preds_reduced = (y_probs_reduced >= t).astype(int)\n",
    "    # Calculate the F1 score for the current predictions and true labels.\n",
    "    f1_scores_reduced.append(f1_score(y_test_bin, preds_reduced))\n",
    "\n",
    "    # Find the index of the threshold that resulted in the highest F1 score.\n",
    "best_idx_reduced = np.argmax(f1_scores_reduced)\n",
    "# Get the best threshold value.\n",
    "best_t_reduced = thresholds[best_idx_reduced]\n",
    "# Get the corresponding best F1 score.\n",
    "best_f1_reduced = f1_scores_reduced[best_idx_reduced]\n",
    "print(f\"Best threshold (Reduced Model, F1-optimized) = {best_t_reduced:.2f}, F1 = {best_f1_reduced:.2f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate scores for plotting (using probabilities from the reduced model)\n",
    "neg_scores_reduced = y_probs_reduced[y_test_bin == 0]\n",
    "pos_scores_reduced = y_probs_reduced[y_test_bin == 1]\n",
    "# Class names are the same as before\n",
    "class_names_reduced = model_reduced.classes_\n",
    "neg_class_name_reduced = class_names_reduced[0]\n",
    "pos_class_name_reduced = class_names_reduced[1]\n",
    "\n",
    "\n",
    "# Plot Predicted Score Distributions with Optimal Threshold (Reduced Data)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.kdeplot(neg_scores_reduced, shade=True, label= (f\"-{neg_class_name_reduced}\"))\n",
    "sns.kdeplot(pos_scores_reduced, shade=True, label= (f\"+{pos_class_name_reduced}\"))\n",
    "\n",
    "# Add a vertical line at the optimal threshold for the reduced model\n",
    "plt.axvline(x=best_t_reduced, color='red', linestyle='--', label=f'Optimal Threshold = {best_t_reduced:.2f}')\n",
    "\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Predicted Score Distributions (KDE) - Reduced Features with Optimal Threshold')\n",
    "plt.legend()\n",
    "plt.savefig('predicted_score_distributions_reduced_with_threshold.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the best threshold to get final predictions (on Reduced Data)\n",
    "final_preds_reduced = (y_probs_reduced >= best_t_reduced).astype(int)\n",
    "\n",
    "print(\"--- Model Evaluation (Reduced Model - Adjusted Threshold) ---\")\n",
    "print(\"Accuracy after adjusting threshold :\", metrics.accuracy_score(y_test_bin, final_preds_reduced))\n",
    "print(\"Confusion Matrix after adjusting threshold:\\n\", confusion_matrix(y_test_bin, final_preds_reduced))\n",
    "print(\"\\nClassification Report after adjusting threshold:\\n\", metrics.classification_report(y_test_bin, final_preds_reduced, target_names=class_names_reduced ))\n",
    "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
    "\n",
    "print(\"--- Analysis Complete ---\")\n",
    "print(\"The script has performed logistic regression, interpreted coefficients,\")\n",
    "print(\"explored feature selection, compared models, and optimized the classification threshold.\")\n",
    "print(\"Review the output and generated plots to understand the model's performance.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Confusion Matrix heatmap when the threshold is adjusted (Reduced Data)\n",
    "cm_thres = confusion_matrix(y_test_bin, final_preds_reduced)\n",
    "labels_reduced = model_reduced.classes_\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_thres, annot=True, fmt='d', cmap='Blues', xticklabels=labels_reduced, yticklabels=labels_reduced)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Logistic Regression (@ Optimal Threshold)')\n",
    "plt.savefig('confusion_matrix_logistic_regression@threshold.png')\n",
    "# plt.show() # Uncomment to display plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LogisticRegression",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
